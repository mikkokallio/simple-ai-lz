# Finlex Ingestion Pipeline - Implementation Options

## Option 1: Azure Functions in Container Apps (Recommended)

### Architecture
```
Container Apps Environment
â”œâ”€â”€ Download Function (HTTP/Timer)
â”œâ”€â”€ Process Function (Blob Trigger)
â””â”€â”€ Shared: Storage, OpenAI, AI Search
```

### Pros
- âœ… Familiar serverless model (event-driven)
- âœ… Built-in triggers (Timer, HTTP, Blob)
- âœ… Auto-scaling based on events
- âœ… Consumption pricing (pay-per-execution)
- âœ… Runs in existing Container Apps Environment
- âœ… Easy local development with Azure Functions Core Tools

### Cons
- âŒ Max execution time limits (230 seconds default, configurable but not ideal for very large archives)
- âŒ More complex for long-running batch jobs
- âŒ Durable Functions needed for orchestration

### When to Choose
- Event-driven processing preferred
- Fast processing per document (< 5 minutes)
- Need built-in trigger bindings
- Team familiar with Azure Functions

---

## Option 2: Container Apps Jobs (Emerging Choice)

### Architecture
```
Container Apps Environment
â”œâ”€â”€ Ingestion Job (CRON scheduled)
â”‚   â”œâ”€â”€ Download step
â”‚   â”œâ”€â”€ Extract step
â”‚   â”œâ”€â”€ Process step
â”‚   â””â”€â”€ Index step
â””â”€â”€ Shared: Storage, OpenAI, AI Search
```

### Pros
- âœ… Purpose-built for batch processing
- âœ… No time limits (can run for hours)
- âœ… CRON-based scheduling built-in
- âœ… Better for long-running tasks
- âœ… Simpler orchestration (single container, multiple steps)
- âœ… Same Container Apps Environment
- âœ… Easier local development (just Docker)
- âœ… Can use any language/framework

### Cons
- âŒ No built-in triggers (only CRON/manual)
- âŒ Need to implement event handling manually
- âŒ Less mature than Functions

### When to Choose
- Batch processing preferred
- Long processing times expected (> 5 minutes)
- Need full control over execution
- Want simpler architecture

---

## Option 3: Standard Container Apps (Alternative)

### Architecture
```
Container Apps Environment
â”œâ”€â”€ Ingestion Service (always-on or scale-to-zero)
â”‚   â””â”€â”€ Express/FastAPI server with endpoints
â”‚       â”œâ”€â”€ POST /ingest (trigger manually)
â”‚       â””â”€â”€ GET /status (check progress)
â””â”€â”€ Background Job Queue (optional)
```

### Pros
- âœ… Maximum flexibility
- âœ… Can add REST API for control
- âœ… Easy to add custom logic
- âœ… Full framework support (Express, FastAPI)

### Cons
- âŒ Need to implement scheduling manually
- âŒ More boilerplate code
- âŒ Overkill for simple batch processing

### When to Choose
- Need REST API for ingestion control
- Want to expose status/monitoring endpoints
- Complex workflow orchestration needed

---

## Recommended Approach: Container Apps Jobs

### Rationale
1. **Batch Nature**: Ingestion is inherently a batch process, not event-driven
2. **Long Running**: Processing large archives may take 30+ minutes
3. **Simplicity**: Single container with sequential steps is easier to understand
4. **Debugging**: Easier to run locally and debug
5. **Flexibility**: Can add orchestration logic without Durable Functions complexity

### Implementation Structure

```typescript
// src/index.ts - Main entry point
async function main() {
  console.log('ğŸš€ Starting Finlex ingestion pipeline...');
  
  try {
    // Step 1: Download
    const archivePath = await downloadFinlexArchive();
    
    // Step 2: Extract
    const documents = await extractArchive(archivePath);
    
    // Step 3: Process
    const chunks = await processDocuments(documents);
    
    // Step 4: Index
    await indexToAISearch(chunks);
    
    console.log('âœ… Ingestion completed successfully');
  } catch (error) {
    console.error('âŒ Ingestion failed:', error);
    process.exit(1);
  }
}

main();
```

### Project Structure
```
finlex-ingestion/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts              # Main orchestrator
â”‚   â”œâ”€â”€ download.ts           # Download from Finlex
â”‚   â”œâ”€â”€ extract.ts            # ZIP extraction
â”‚   â”œâ”€â”€ process.ts            # Document parsing & chunking
â”‚   â”œâ”€â”€ embed.ts              # Generate embeddings
â”‚   â”œâ”€â”€ index.ts              # Upload to AI Search
â”‚   â”œâ”€â”€ models/               # TypeScript interfaces
â”‚   â”‚   â”œâ”€â”€ Document.ts
â”‚   â”‚   â”œâ”€â”€ Chunk.ts
â”‚   â”‚   â””â”€â”€ SearchDocument.ts
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ storage.ts        # Blob Storage helper
â”‚       â”œâ”€â”€ openai.ts         # OpenAI client
â”‚       â”œâ”€â”€ search.ts         # AI Search client
â”‚       â””â”€â”€ logger.ts         # Logging utility
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ job.bicep             # Container Apps Job
â”‚   â”œâ”€â”€ search.bicep          # AI Search service
â”‚   â””â”€â”€ storage.bicep         # Storage containers
â””â”€â”€ README.md
```

---

## Infrastructure Components Needed

### New Resources

1. **Azure AI Search Service**
   ```bicep
   resource search 'Microsoft.Search/searchServices@2023-11-01' = {
     name: 'search-${uniqueString(resourceGroup().id)}'
     location: location
     sku: {
       name: 'basic'  // S1 for production
     }
     properties: {
       replicaCount: 1
       partitionCount: 1
       semanticSearch: 'standard'  // Enable semantic search
     }
   }
   ```

2. **Storage Containers** (in existing account)
   - `finlex-raw`: Downloaded ZIP archives
   - `finlex-processed`: Extracted documents

3. **Container Apps Job**
   ```bicep
   resource job 'Microsoft.App/jobs@2023-05-01' = {
     name: 'job-finlex-ingestion'
     location: location
     properties: {
       environmentId: containerAppsEnvId
       configuration: {
         triggerType: 'Schedule'
         scheduleTriggerConfig: {
           cronExpression: '0 0 2 * * *'  // Daily at 2 AM
           parallelism: 1
           replicaCompletionCount: 1
         }
         replicaTimeout: 7200  // 2 hours max
       }
       template: {
         containers: [
           {
             name: 'finlex-ingestion'
             image: '${acr}.azurecr.io/finlex-ingestion:latest'
             resources: {
               cpu: json('1.0')
               memory: '2Gi'
             }
             env: [/* environment variables */]
           }
         ]
       }
     }
   }
   ```

### Shared Resources (Already Exist)
- Container Apps Environment
- Storage Account
- Azure OpenAI
- Application Insights
- Key Vault

---

## Next Steps

1. **Prototype**: Build simple Node.js script to download and parse sample Finlex data
2. **Containerize**: Create Dockerfile and test locally
3. **Infrastructure**: Create Bicep modules for AI Search and Container Apps Job
4. **Deploy**: Test end-to-end in Azure
5. **Monitor**: Set up Application Insights dashboards
6. **Optimize**: Add caching, better error handling, incremental updates
